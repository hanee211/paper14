{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced dynamic seq2seq with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder is bidirectional now. Decoder is implemented using `tf.nn.raw_rnn`. \n",
    "It feeds previously generated tokens during training as inputs, instead of target sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UPDATE (16.02.2017)**: I learned some things after I wrote this tutorial. In particular:\n",
    " - [DONE] Replacing projection (one-hot encoding followed by linear layer) with embedding (indexing weights of linear layer directly) is more efficient.\n",
    " - When decoding, feeding previously generated tokens as inputs adds robustness to model's errors. However feeding ground truth speeds up training. Apperantly best practice is to mix both randomly when training.\n",
    "\n",
    "I will update tutorial to reflect this at some point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import helpers\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "EOS = 1\n",
    "\n",
    "vocab_size = 10\n",
    "input_embedding_size = 20\n",
    "\n",
    "encoder_hidden_units = 20\n",
    "decoder_hidden_units = encoder_hidden_units * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"decoder_targets_5:0\", shape=(?, ?), dtype=int32)\n",
      "Tensor(\"encoder_inputs_length_5:0\", shape=(?,), dtype=int32)\n",
      "Tensor(\"add_2:0\", shape=(?,), dtype=int32)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'Tensor' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-22b6faa75664>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder_inputs_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder_inputs_length\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder_inputs_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'Tensor' has no len()"
     ]
    }
   ],
   "source": [
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length')\n",
    "\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')\n",
    "print(decoder_targets)\n",
    "print(encoder_inputs_length)\n",
    "print(encoder_inputs_length + 3)\n",
    "print(len(encoder_inputs_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we elected to manually feed `decoder_inputs` to better understand what is going on. Here we implement decoder with `tf.nn.raw_rnn` and will construct `decoder_inputs` step by step in the loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "Setup embeddings (see tutorial 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "We are replacing unidirectional `tf.nn.dynamic_rnn` with `tf.nn.bidirectional_dynamic_rnn` as the encoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_cell = LSTMCell(encoder_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "((encoder_fw_outputs,\n",
    "  encoder_bw_outputs),\n",
    " (encoder_fw_final_state,\n",
    "  encoder_bw_final_state)) = (\n",
    "    tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                    cell_bw=encoder_cell,\n",
    "                                    inputs=encoder_inputs_embedded,\n",
    "                                    sequence_length=encoder_inputs_length,\n",
    "                                    dtype=tf.float32, time_major=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'bidirectional_rnn/fw/fw/TensorArrayStack/TensorArrayGatherV3:0' shape=(?, ?, 20) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_fw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ReverseSequence:0' shape=(?, ?, 20) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_bw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_2:0' shape=(?, 20) dtype=float32>, h=<tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 20) dtype=float32>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_fw_final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'bidirectional_rnn/bw/bw/while/Exit_2:0' shape=(?, 20) dtype=float32>, h=<tf.Tensor 'bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(?, 20) dtype=float32>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_bw_final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have to concatenate forward and backward outputs and state. In this case we will not discard outputs, they would be used for attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-f9e83e7bedda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[1;31m#print(sess.run(encoder_final_state))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[1;31m#print(sess.run(tf.shape(encoder_final_state)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder_final_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32md:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32md:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    936\u001b[0m                 ' to a larger type (e.g. int64).')\n\u001b[1;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m           \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32md:\\Users\\home\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \"\"\"\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n",
    "\n",
    "encoder_final_state_c = tf.concat(\n",
    "    (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "encoder_final_state_h = tf.concat(\n",
    "    (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "encoder_final_state = LSTMStateTuple(\n",
    "    c=encoder_final_state_c,\n",
    "    h=encoder_final_state_h\n",
    ")\n",
    "\n",
    "#print(sess.run(encoder_final_state))\n",
    "#print(sess.run(tf.shape(encoder_final_state)))\n",
    "print(sess.run(tf.unstack(tf.shape(encoder_final_state)), {encoder_inputs:[[[3,4],[5]]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoder_cell = LSTMCell(decoder_hidden_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time and batch dimensions are dynamic, i.e. they can change in runtime, from batch to batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_max_time, batch_size = tf.unstack(tf.shape(encoder_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to decide how far to run decoder. There are several options for stopping criteria:\n",
    "- Stop after specified number of unrolling steps\n",
    "- Stop after model produced <EOS> token\n",
    "\n",
    "The choice will likely be time-dependant. In legacy `translate` tutorial we can see that decoder unrolls for `len(encoder_input)+10` to allow for possibly longer translated sequence. Here we are doing a toy copy task, so how about we unroll decoder for `len(encoder_input)+2`, to allow model some room to make mistakes over 2 additional steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoder_lengths = encoder_inputs_length + 3\n",
    "# +2 additional steps, +1 leading <EOS> token for decoder inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output projection\n",
    "\n",
    "Decoder will contain manually specified by us transition step:\n",
    "```\n",
    "output(t) -> output projection(t) -> prediction(t) (argmax) -> input embedding(t+1) -> input(t+1)\n",
    "```\n",
    "\n",
    "In tutorial 1, we used `tf.contrib.layers.linear` layer to initialize weights and biases and apply operation for us. This is convenient, however now we need to specify parameters `W` and `b`  of the output layer in global scope, and apply them at every step of the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_uniform([decoder_hidden_units, vocab_size], -1, 1), dtype=tf.float32)\n",
    "b = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder via `tf.nn.raw_rnn`\n",
    "\n",
    "`tf.nn.dynamic_rnn` allows for easy RNN construction, but is limited. \n",
    "\n",
    "For example, a nice way to increase robustness of the model is to feed as decoder inputs tokens that it previously generated, instead of shifted true sequence.\n",
    "\n",
    "![seq2seq-feed-previous](pictures/2-seq2seq-feed-previous.png)\n",
    "*Image borrowed from http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First prepare tokens. Decoder would operate on column vectors of shape `(batch_size,)` representing single time steps of the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert EOS == 1 and PAD == 0\n",
    "\n",
    "eos_time_slice = tf.ones([batch_size], dtype=tf.int32, name='EOS')\n",
    "pad_time_slice = tf.zeros([batch_size], dtype=tf.int32, name='PAD')\n",
    "\n",
    "eos_step_embedded = tf.nn.embedding_lookup(embeddings, eos_time_slice)\n",
    "pad_step_embedded = tf.nn.embedding_lookup(embeddings, pad_time_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the tricky part.\n",
    "\n",
    "Remember that standard `tf.nn.dynamic_rnn` requires all inputs `(t, ..., t+n)` be passed in advance as a single tensor. \"Dynamic\" part of its name refers to the fact that `n` can change from batch to batch.\n",
    "\n",
    "Now, what if we want to implement more complex mechanic like when we want decoder to receive previously generated tokens as input at every timestamp (instead of lagged target sequence)? Or when we want to implement soft attention, where at every timestep we add additional fixed-len representation, derived from query produced by previous step's hidden state? `tf.nn.raw_rnn` is a way to solve this problem.\n",
    "\n",
    "Main part of specifying RNN with `tf.nn.raw_rnn` is *loop transition function*. It defines inputs of step `t` given outputs and state of step `t-1`.\n",
    "\n",
    "Loop transition function is a mapping `(time, previous_cell_output, previous_cell_state, previous_loop_state) -> (elements_finished, input, cell_state, output, loop_state)`. It is called *before* RNNCell to prepare its inputs and state. Everything is a Tensor except for initial call at time=0 when everything is `None` (except `time`).\n",
    "\n",
    "Note that decoder inputs are returned from the transition function but passed into it. You are supposed to index inputs manually using `time` Tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop transition function is called two times:\n",
    " 1. Initial call at time=0 to provide initial cell_state and input to RNN.\n",
    " 2. Transition call for all following timesteps where you define transition between two adjacent steps.\n",
    "\n",
    "Lets define both cases separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop initial state is function of only `encoder_final_state` and embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_fn_initial():\n",
    "    initial_elements_finished = (0 >= decoder_lengths)  # all False at the initial step\n",
    "    initial_input = eos_step_embedded\n",
    "    initial_cell_state = encoder_final_state\n",
    "    initial_cell_output = None\n",
    "    initial_loop_state = None  # we don't need to pass any additional information\n",
    "    return (initial_elements_finished,\n",
    "            initial_input,\n",
    "            initial_cell_state,\n",
    "            initial_cell_output,\n",
    "            initial_loop_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define transition function such that previously generated token (as judged in greedy manner by `argmax` over output projection) is passed as next input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):\n",
    "\n",
    "    def get_next_input():\n",
    "        output_logits = tf.add(tf.matmul(previous_output, W), b)\n",
    "        prediction = tf.argmax(output_logits, axis=1)\n",
    "        next_input = tf.nn.embedding_lookup(embeddings, prediction)\n",
    "        return next_input\n",
    "    \n",
    "    elements_finished = (time >= decoder_lengths) # this operation produces boolean tensor of [batch_size]\n",
    "                                                  # defining if corresponding sequence has ended\n",
    "\n",
    "    finished = tf.reduce_all(elements_finished) # -> boolean scalar\n",
    "    input = tf.cond(finished, lambda: pad_step_embedded, get_next_input)\n",
    "    state = previous_state\n",
    "    output = previous_output\n",
    "    loop_state = None\n",
    "\n",
    "    return (elements_finished, \n",
    "            input,\n",
    "            state,\n",
    "            output,\n",
    "            loop_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine initializer and transition functions and create raw_rnn.\n",
    "\n",
    "Note that while all operations above are defined with TF's control flow and reduction ops, here we rely on checking if state is `None` to determine if it is an initializer call or transition call. This is not very clean API and might be changed in the future (indeed, `tf.nn.raw_rnn`'s doc contains warning that API is experimental)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loop_fn(time, previous_output, previous_state, previous_loop_state):\n",
    "    if previous_state is None:    # time == 0\n",
    "        assert previous_output is None and previous_state is None\n",
    "        return loop_fn_initial()\n",
    "    else:\n",
    "        return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)\n",
    "\n",
    "decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn)\n",
    "decoder_outputs = decoder_outputs_ta.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'TensorArrayStack/TensorArrayGatherV3:0' shape=(?, ?, 40) dtype=float32>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do output projection, we have to temporarilly flatten `decoder_outputs` from `[max_steps, batch_size, hidden_dim]` to `[max_steps*batch_size, hidden_dim]`, as `tf.matmul` needs rank-2 tensors at most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
    "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_dim))\n",
    "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
    "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "decoder_prediction = tf.argmax(decoder_logits, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN outputs tensor of shape `[max_time, batch_size, hidden_units]` which projection layer maps onto `[max_time, batch_size, vocab_size]`. `vocab_size` part of the shape is static, while `max_time` and `batch_size` is dynamic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on the toy task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the copy task — given a random sequence of integers from a `vocabulary`, learn to memorize and reproduce input sequence. Because sequences are random, they do not contain any structure, unlike natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head of the batch:\n",
      "[9, 6, 6, 8]\n",
      "[4, 4, 6]\n",
      "[3, 8, 7, 5, 8]\n",
      "[4, 3, 8, 3, 8, 4, 3]\n",
      "[2, 2, 6]\n",
      "[3, 7, 7, 6, 4, 9]\n",
      "[3, 6, 9]\n",
      "[5, 8, 8, 7, 4, 7]\n",
      "[8, 5, 5, 7, 4, 8, 6, 2]\n",
      "[7, 4, 5, 3, 5, 2, 5]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "batches = helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                   vocab_lower=2, vocab_upper=10,\n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "print('head of the batch:')\n",
    "for seq in next(batches)[:10]:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_feed():\n",
    "    batch = next(batches)\n",
    "    encoder_inputs_, encoder_input_lengths_ = helpers.batch(batch)\n",
    "    decoder_targets_, _ = helpers.batch(\n",
    "        [(sequence) + [EOS] + [PAD] * 2 for sequence in batch]\n",
    "    )\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_,\n",
    "        encoder_inputs_length: encoder_input_lengths_,\n",
    "        decoder_targets: decoder_targets_,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_track = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Cannot assign a device to node 'Adam/update_Variable/sub_3': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/GPU:0'\nColocation Debug Info:\nColocation group had the following types and devices: \nAssign: GPU CPU \nConst: GPU CPU \nEnter: GPU CPU \nSub: GPU CPU \nMul: GPU CPU \nVariableV2: GPU CPU \nIdentity: GPU CPU \nSqrt: GPU CPU \nRealDiv: GPU CPU \nSwitch: GPU CPU \nGather: GPU CPU \nShape: GPU CPU \nUnique: CPU \nStridedSlice: GPU CPU \nUnsortedSegmentSum: GPU CPU \nScatterAdd: GPU CPU \nAdd: GPU CPU \nAssignSub: GPU CPU \nNoOp: GPU CPU \n\t [[Node: Adam/update_Variable/sub_3 = Sub[T=DT_FLOAT, _class=[\"loc:@Variable\"]](Adam/update_Variable/sub_3/x, Adam/beta2)]]\n\nCaused by op 'Adam/update_Variable/sub_3', defined at:\n  File \"d:\\Users\\home\\Anaconda3\\lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 653, in launch_instance\n    app.start()\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-25-05f6ec0fbbf9>\", line 7, in <module>\n    train_op = tf.train.AdamOptimizer().minimize(loss)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 289, in minimize\n    name=name)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 413, in apply_gradients\n    update_ops.append(processor.update_op(self, grad))\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 66, in update_op\n    return optimizer._apply_sparse_duplicate_indices(g, self._v)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 557, in _apply_sparse_duplicate_indices\n    return self._apply_sparse(gradient_no_duplicate_indices, var)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\adam.py\", line 156, in _apply_sparse\n    v_scaled_g_values = (grad.values * grad.values) * (1 - beta2_t)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 808, in r_binary_op_wrapper\n    return func(x, y, name=name)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 2775, in _sub\n    result = _op_def_lib.apply_op(\"Sub\", x=x, y=y, name=name)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Cannot assign a device to node 'Adam/update_Variable/sub_3': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/GPU:0'\nColocation Debug Info:\nColocation group had the following types and devices: \nAssign: GPU CPU \nConst: GPU CPU \nEnter: GPU CPU \nSub: GPU CPU \nMul: GPU CPU \nVariableV2: GPU CPU \nIdentity: GPU CPU \nSqrt: GPU CPU \nRealDiv: GPU CPU \nSwitch: GPU CPU \nGather: GPU CPU \nShape: GPU CPU \nUnique: CPU \nStridedSlice: GPU CPU \nUnsortedSegmentSum: GPU CPU \nScatterAdd: GPU CPU \nAdd: GPU CPU \nAssignSub: GPU CPU \nNoOp: GPU CPU \n\t [[Node: Adam/update_Variable/sub_3 = Sub[T=DT_FLOAT, _class=[\"loc:@Variable\"]](Adam/update_Variable/sub_3/x, Adam/beta2)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32md:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32md:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32md:\\Users\\home\\Anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32md:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot assign a device to node 'Adam/update_Variable/sub_3': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/GPU:0'\nColocation Debug Info:\nColocation group had the following types and devices: \nAssign: GPU CPU \nConst: GPU CPU \nEnter: GPU CPU \nSub: GPU CPU \nMul: GPU CPU \nVariableV2: GPU CPU \nIdentity: GPU CPU \nSqrt: GPU CPU \nRealDiv: GPU CPU \nSwitch: GPU CPU \nGather: GPU CPU \nShape: GPU CPU \nUnique: CPU \nStridedSlice: GPU CPU \nUnsortedSegmentSum: GPU CPU \nScatterAdd: GPU CPU \nAdd: GPU CPU \nAssignSub: GPU CPU \nNoOp: GPU CPU \n\t [[Node: Adam/update_Variable/sub_3 = Sub[T=DT_FLOAT, _class=[\"loc:@Variable\"]](Adam/update_Variable/sub_3/x, Adam/beta2)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-d7706e679a79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mfd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_feed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mloss_track\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32md:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32md:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32md:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32md:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot assign a device to node 'Adam/update_Variable/sub_3': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/GPU:0'\nColocation Debug Info:\nColocation group had the following types and devices: \nAssign: GPU CPU \nConst: GPU CPU \nEnter: GPU CPU \nSub: GPU CPU \nMul: GPU CPU \nVariableV2: GPU CPU \nIdentity: GPU CPU \nSqrt: GPU CPU \nRealDiv: GPU CPU \nSwitch: GPU CPU \nGather: GPU CPU \nShape: GPU CPU \nUnique: CPU \nStridedSlice: GPU CPU \nUnsortedSegmentSum: GPU CPU \nScatterAdd: GPU CPU \nAdd: GPU CPU \nAssignSub: GPU CPU \nNoOp: GPU CPU \n\t [[Node: Adam/update_Variable/sub_3 = Sub[T=DT_FLOAT, _class=[\"loc:@Variable\"]](Adam/update_Variable/sub_3/x, Adam/beta2)]]\n\nCaused by op 'Adam/update_Variable/sub_3', defined at:\n  File \"d:\\Users\\home\\Anaconda3\\lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 653, in launch_instance\n    app.start()\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-25-05f6ec0fbbf9>\", line 7, in <module>\n    train_op = tf.train.AdamOptimizer().minimize(loss)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 289, in minimize\n    name=name)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 413, in apply_gradients\n    update_ops.append(processor.update_op(self, grad))\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 66, in update_op\n    return optimizer._apply_sparse_duplicate_indices(g, self._v)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 557, in _apply_sparse_duplicate_indices\n    return self._apply_sparse(gradient_no_duplicate_indices, var)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\adam.py\", line 156, in _apply_sparse\n    v_scaled_g_values = (grad.values * grad.values) * (1 - beta2_t)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 808, in r_binary_op_wrapper\n    return func(x, y, name=name)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 2775, in _sub\n    result = _op_def_lib.apply_op(\"Sub\", x=x, y=y, name=name)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"d:\\Users\\home\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Cannot assign a device to node 'Adam/update_Variable/sub_3': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/GPU:0'\nColocation Debug Info:\nColocation group had the following types and devices: \nAssign: GPU CPU \nConst: GPU CPU \nEnter: GPU CPU \nSub: GPU CPU \nMul: GPU CPU \nVariableV2: GPU CPU \nIdentity: GPU CPU \nSqrt: GPU CPU \nRealDiv: GPU CPU \nSwitch: GPU CPU \nGather: GPU CPU \nShape: GPU CPU \nUnique: CPU \nStridedSlice: GPU CPU \nUnsortedSegmentSum: GPU CPU \nScatterAdd: GPU CPU \nAdd: GPU CPU \nAssignSub: GPU CPU \nNoOp: GPU CPU \n\t [[Node: Adam/update_Variable/sub_3 = Sub[T=DT_FLOAT, _class=[\"loc:@Variable\"]](Adam/update_Variable/sub_3/x, Adam/beta2)]]\n"
     ]
    }
   ],
   "source": [
    "max_batches = 3001\n",
    "batches_in_epoch = 1000\n",
    "\n",
    "try:\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed()\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "\n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(decoder_prediction, fd)\n",
    "            for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format(inp))\n",
    "                print('    predicted > {}'.format(pred))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-9166d688c1d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_track\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loss {:.4f} after {} examples (batch_size={})'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_track\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_track\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiQAAAFkCAYAAAAQQyCBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e9c5a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
